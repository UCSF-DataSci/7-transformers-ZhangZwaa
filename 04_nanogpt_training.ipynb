{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bd44fc0",
   "metadata": {},
   "source": [
    "# Part 4: Optional - nanoGPT Training on Healthcare Text Data\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this optional part, you'll train a small GPT model (nanoGPT) on healthcare text data. This will give you hands-on experience with training language models from scratch and understanding their capabilities and limitations. You'll use the Synthetic Mention Corpora for Disease Entity Recognition, which contains 128,000 disease mentions generated by an LLM.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Understand the architecture of small language models\n",
    "- Prepare text data for language model training\n",
    "- Train a nanoGPT model on domain-specific data\n",
    "- Evaluate model performance and generated text quality\n",
    "- Compare with larger pre-trained models\n",
    "\n",
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7549f155",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy>=1.20.0 in d:\\homework\\.venv\\lib\\site-packages (from -r requirements.txt (line 2)) (2.1.3)\n",
      "Requirement already satisfied: pandas>=1.3.0 in d:\\homework\\.venv\\lib\\site-packages (from -r requirements.txt (line 3)) (2.2.3)\n",
      "Requirement already satisfied: matplotlib>=3.4.0 in d:\\homework\\.venv\\lib\\site-packages (from -r requirements.txt (line 4)) (3.10.3)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in d:\\homework\\.venv\\lib\\site-packages (from -r requirements.txt (line 5)) (0.13.2)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in d:\\homework\\.venv\\lib\\site-packages (from -r requirements.txt (line 6)) (1.6.1)\n",
      "Requirement already satisfied: tqdm>=4.62.0 in d:\\homework\\.venv\\lib\\site-packages (from -r requirements.txt (line 7)) (4.67.1)\n",
      "Requirement already satisfied: tensorflow>=2.8.0 in d:\\homework\\.venv\\lib\\site-packages (from -r requirements.txt (line 10)) (2.19.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in d:\\homework\\.venv\\lib\\site-packages (from -r requirements.txt (line 11)) (2.7.0+cu126)\n",
      "Requirement already satisfied: transformers>=4.18.0 in d:\\homework\\.venv\\lib\\site-packages (from -r requirements.txt (line 12)) (4.52.4)\n",
      "Requirement already satisfied: requests>=2.27.0 in d:\\homework\\.venv\\lib\\site-packages (from -r requirements.txt (line 15)) (2.32.3)\n",
      "Requirement already satisfied: huggingface_hub>=0.5.0 in d:\\homework\\.venv\\lib\\site-packages (from -r requirements.txt (line 16)) (0.32.4)\n",
      "Requirement already satisfied: accelerate>=0.12.0 in d:\\homework\\.venv\\lib\\site-packages (from -r requirements.txt (line 17)) (1.7.0)\n",
      "Requirement already satisfied: sentencepiece>=0.1.96 in d:\\homework\\.venv\\lib\\site-packages (from -r requirements.txt (line 18)) (0.2.0)\n",
      "Requirement already satisfied: tokenizers>=0.12.1 in d:\\homework\\.venv\\lib\\site-packages (from -r requirements.txt (line 19)) (0.21.1)\n",
      "Requirement already satisfied: datasets>=2.0.0 in d:\\homework\\.venv\\lib\\site-packages (from -r requirements.txt (line 22)) (3.6.0)\n",
      "Requirement already satisfied: regex>=2022.3.15 in d:\\homework\\.venv\\lib\\site-packages (from -r requirements.txt (line 23)) (2024.11.6)\n",
      "Requirement already satisfied: plotly>=5.6.0 in d:\\homework\\.venv\\lib\\site-packages (from -r requirements.txt (line 26)) (6.1.2)\n",
      "Requirement already satisfied: wandb>=0.12.0 in d:\\homework\\.venv\\lib\\site-packages (from -r requirements.txt (line 29)) (0.20.1)\n",
      "Requirement already satisfied: pytest>=7.0.0 in d:\\homework\\.venv\\lib\\site-packages (from -r requirements.txt (line 30)) (8.4.0)\n",
      "Requirement already satisfied: jupytext>=1.13.8 in d:\\homework\\.venv\\lib\\site-packages (from -r requirements.txt (line 31)) (1.17.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\homework\\.venv\\lib\\site-packages (from pandas>=1.3.0->-r requirements.txt (line 3)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\homework\\.venv\\lib\\site-packages (from pandas>=1.3.0->-r requirements.txt (line 3)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\homework\\.venv\\lib\\site-packages (from pandas>=1.3.0->-r requirements.txt (line 3)) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\homework\\.venv\\lib\\site-packages (from matplotlib>=3.4.0->-r requirements.txt (line 4)) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\homework\\.venv\\lib\\site-packages (from matplotlib>=3.4.0->-r requirements.txt (line 4)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\homework\\.venv\\lib\\site-packages (from matplotlib>=3.4.0->-r requirements.txt (line 4)) (4.58.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\homework\\.venv\\lib\\site-packages (from matplotlib>=3.4.0->-r requirements.txt (line 4)) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\homework\\.venv\\lib\\site-packages (from matplotlib>=3.4.0->-r requirements.txt (line 4)) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in d:\\homework\\.venv\\lib\\site-packages (from matplotlib>=3.4.0->-r requirements.txt (line 4)) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\homework\\.venv\\lib\\site-packages (from matplotlib>=3.4.0->-r requirements.txt (line 4)) (3.2.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in d:\\homework\\.venv\\lib\\site-packages (from scikit-learn>=1.0.0->-r requirements.txt (line 6)) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\homework\\.venv\\lib\\site-packages (from scikit-learn>=1.0.0->-r requirements.txt (line 6)) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\homework\\.venv\\lib\\site-packages (from scikit-learn>=1.0.0->-r requirements.txt (line 6)) (3.6.0)\n",
      "Requirement already satisfied: colorama in d:\\homework\\.venv\\lib\\site-packages (from tqdm>=4.62.0->-r requirements.txt (line 7)) (0.4.6)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in d:\\homework\\.venv\\lib\\site-packages (from tensorflow>=2.8.0->-r requirements.txt (line 10)) (2.3.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in d:\\homework\\.venv\\lib\\site-packages (from tensorflow>=2.8.0->-r requirements.txt (line 10)) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in d:\\homework\\.venv\\lib\\site-packages (from tensorflow>=2.8.0->-r requirements.txt (line 10)) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in d:\\homework\\.venv\\lib\\site-packages (from tensorflow>=2.8.0->-r requirements.txt (line 10)) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in d:\\homework\\.venv\\lib\\site-packages (from tensorflow>=2.8.0->-r requirements.txt (line 10)) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in d:\\homework\\.venv\\lib\\site-packages (from tensorflow>=2.8.0->-r requirements.txt (line 10)) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in d:\\homework\\.venv\\lib\\site-packages (from tensorflow>=2.8.0->-r requirements.txt (line 10)) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in d:\\homework\\.venv\\lib\\site-packages (from tensorflow>=2.8.0->-r requirements.txt (line 10)) (5.29.5)\n",
      "Requirement already satisfied: setuptools in d:\\homework\\.venv\\lib\\site-packages (from tensorflow>=2.8.0->-r requirements.txt (line 10)) (70.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in d:\\homework\\.venv\\lib\\site-packages (from tensorflow>=2.8.0->-r requirements.txt (line 10)) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in d:\\homework\\.venv\\lib\\site-packages (from tensorflow>=2.8.0->-r requirements.txt (line 10)) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in d:\\homework\\.venv\\lib\\site-packages (from tensorflow>=2.8.0->-r requirements.txt (line 10)) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in d:\\homework\\.venv\\lib\\site-packages (from tensorflow>=2.8.0->-r requirements.txt (line 10)) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in d:\\homework\\.venv\\lib\\site-packages (from tensorflow>=2.8.0->-r requirements.txt (line 10)) (1.73.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in d:\\homework\\.venv\\lib\\site-packages (from tensorflow>=2.8.0->-r requirements.txt (line 10)) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in d:\\homework\\.venv\\lib\\site-packages (from tensorflow>=2.8.0->-r requirements.txt (line 10)) (3.10.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in d:\\homework\\.venv\\lib\\site-packages (from tensorflow>=2.8.0->-r requirements.txt (line 10)) (3.14.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in d:\\homework\\.venv\\lib\\site-packages (from tensorflow>=2.8.0->-r requirements.txt (line 10)) (0.5.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\homework\\.venv\\lib\\site-packages (from requests>=2.27.0->-r requirements.txt (line 15)) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\homework\\.venv\\lib\\site-packages (from requests>=2.27.0->-r requirements.txt (line 15)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\homework\\.venv\\lib\\site-packages (from requests>=2.27.0->-r requirements.txt (line 15)) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\homework\\.venv\\lib\\site-packages (from requests>=2.27.0->-r requirements.txt (line 15)) (2025.4.26)\n",
      "Requirement already satisfied: markdown>=2.6.8 in d:\\homework\\.venv\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow>=2.8.0->-r requirements.txt (line 10)) (3.8)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in d:\\homework\\.venv\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow>=2.8.0->-r requirements.txt (line 10)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in d:\\homework\\.venv\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow>=2.8.0->-r requirements.txt (line 10)) (3.1.3)\n",
      "Requirement already satisfied: filelock in d:\\homework\\.venv\\lib\\site-packages (from torch>=1.10.0->-r requirements.txt (line 11)) (3.13.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in d:\\homework\\.venv\\lib\\site-packages (from torch>=1.10.0->-r requirements.txt (line 11)) (1.13.3)\n",
      "Requirement already satisfied: networkx in d:\\homework\\.venv\\lib\\site-packages (from torch>=1.10.0->-r requirements.txt (line 11)) (3.3)\n",
      "Requirement already satisfied: jinja2 in d:\\homework\\.venv\\lib\\site-packages (from torch>=1.10.0->-r requirements.txt (line 11)) (3.1.4)\n",
      "Requirement already satisfied: fsspec in d:\\homework\\.venv\\lib\\site-packages (from torch>=1.10.0->-r requirements.txt (line 11)) (2024.6.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\homework\\.venv\\lib\\site-packages (from transformers>=4.18.0->-r requirements.txt (line 12)) (6.0.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in d:\\homework\\.venv\\lib\\site-packages (from transformers>=4.18.0->-r requirements.txt (line 12)) (0.5.3)\n",
      "Requirement already satisfied: psutil in d:\\homework\\.venv\\lib\\site-packages (from accelerate>=0.12.0->-r requirements.txt (line 17)) (7.0.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in d:\\homework\\.venv\\lib\\site-packages (from datasets>=2.0.0->-r requirements.txt (line 22)) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in d:\\homework\\.venv\\lib\\site-packages (from datasets>=2.0.0->-r requirements.txt (line 22)) (0.3.8)\n",
      "Requirement already satisfied: xxhash in d:\\homework\\.venv\\lib\\site-packages (from datasets>=2.0.0->-r requirements.txt (line 22)) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in d:\\homework\\.venv\\lib\\site-packages (from datasets>=2.0.0->-r requirements.txt (line 22)) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in d:\\homework\\.venv\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.0.0->-r requirements.txt (line 22)) (3.12.11)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in d:\\homework\\.venv\\lib\\site-packages (from plotly>=5.6.0->-r requirements.txt (line 26)) (1.42.0)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in d:\\homework\\.venv\\lib\\site-packages (from wandb>=0.12.0->-r requirements.txt (line 29)) (8.2.1)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in d:\\homework\\.venv\\lib\\site-packages (from wandb>=0.12.0->-r requirements.txt (line 29)) (3.1.44)\n",
      "Requirement already satisfied: platformdirs in d:\\homework\\.venv\\lib\\site-packages (from wandb>=0.12.0->-r requirements.txt (line 29)) (4.3.8)\n",
      "Requirement already satisfied: pydantic<3 in d:\\homework\\.venv\\lib\\site-packages (from wandb>=0.12.0->-r requirements.txt (line 29)) (2.11.5)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in d:\\homework\\.venv\\lib\\site-packages (from wandb>=0.12.0->-r requirements.txt (line 29)) (2.29.1)\n",
      "Requirement already satisfied: setproctitle in d:\\homework\\.venv\\lib\\site-packages (from wandb>=0.12.0->-r requirements.txt (line 29)) (1.3.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\homework\\.venv\\lib\\site-packages (from pydantic<3->wandb>=0.12.0->-r requirements.txt (line 29)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in d:\\homework\\.venv\\lib\\site-packages (from pydantic<3->wandb>=0.12.0->-r requirements.txt (line 29)) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in d:\\homework\\.venv\\lib\\site-packages (from pydantic<3->wandb>=0.12.0->-r requirements.txt (line 29)) (0.4.1)\n",
      "Requirement already satisfied: iniconfig>=1 in d:\\homework\\.venv\\lib\\site-packages (from pytest>=7.0.0->-r requirements.txt (line 30)) (2.1.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in d:\\homework\\.venv\\lib\\site-packages (from pytest>=7.0.0->-r requirements.txt (line 30)) (1.6.0)\n",
      "Requirement already satisfied: pygments>=2.7.2 in d:\\homework\\.venv\\lib\\site-packages (from pytest>=7.0.0->-r requirements.txt (line 30)) (2.19.1)\n",
      "Requirement already satisfied: markdown-it-py>=1.0 in d:\\homework\\.venv\\lib\\site-packages (from jupytext>=1.13.8->-r requirements.txt (line 31)) (3.0.0)\n",
      "Requirement already satisfied: mdit-py-plugins in d:\\homework\\.venv\\lib\\site-packages (from jupytext>=1.13.8->-r requirements.txt (line 31)) (0.4.2)\n",
      "Requirement already satisfied: nbformat in d:\\homework\\.venv\\lib\\site-packages (from jupytext>=1.13.8->-r requirements.txt (line 31)) (5.10.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in d:\\homework\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.0.0->-r requirements.txt (line 22)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\homework\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.0.0->-r requirements.txt (line 22)) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\homework\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.0.0->-r requirements.txt (line 22)) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\homework\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.0.0->-r requirements.txt (line 22)) (1.6.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\homework\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.0.0->-r requirements.txt (line 22)) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\homework\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.0.0->-r requirements.txt (line 22)) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\homework\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.0.0->-r requirements.txt (line 22)) (1.20.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in d:\\homework\\.venv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow>=2.8.0->-r requirements.txt (line 10)) (0.45.1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in d:\\homework\\.venv\\lib\\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.12.0->-r requirements.txt (line 29)) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in d:\\homework\\.venv\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.12.0->-r requirements.txt (line 29)) (5.0.2)\n",
      "Requirement already satisfied: rich in d:\\homework\\.venv\\lib\\site-packages (from keras>=3.5.0->tensorflow>=2.8.0->-r requirements.txt (line 10)) (14.0.0)\n",
      "Requirement already satisfied: namex in d:\\homework\\.venv\\lib\\site-packages (from keras>=3.5.0->tensorflow>=2.8.0->-r requirements.txt (line 10)) (0.1.0)\n",
      "Requirement already satisfied: optree in d:\\homework\\.venv\\lib\\site-packages (from keras>=3.5.0->tensorflow>=2.8.0->-r requirements.txt (line 10)) (0.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\homework\\.venv\\lib\\site-packages (from markdown-it-py>=1.0->jupytext>=1.13.8->-r requirements.txt (line 31)) (0.1.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\homework\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch>=1.10.0->-r requirements.txt (line 11)) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in d:\\homework\\.venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow>=2.8.0->-r requirements.txt (line 10)) (2.1.5)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in d:\\homework\\.venv\\lib\\site-packages (from nbformat->jupytext>=1.13.8->-r requirements.txt (line 31)) (2.21.1)\n",
      "Requirement already satisfied: jsonschema>=2.6 in d:\\homework\\.venv\\lib\\site-packages (from nbformat->jupytext>=1.13.8->-r requirements.txt (line 31)) (4.24.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in d:\\homework\\.venv\\lib\\site-packages (from nbformat->jupytext>=1.13.8->-r requirements.txt (line 31)) (5.7.2)\n",
      "Requirement already satisfied: traitlets>=5.1 in d:\\homework\\.venv\\lib\\site-packages (from nbformat->jupytext>=1.13.8->-r requirements.txt (line 31)) (5.14.3)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in d:\\homework\\.venv\\lib\\site-packages (from jsonschema>=2.6->nbformat->jupytext>=1.13.8->-r requirements.txt (line 31)) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in d:\\homework\\.venv\\lib\\site-packages (from jsonschema>=2.6->nbformat->jupytext>=1.13.8->-r requirements.txt (line 31)) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in d:\\homework\\.venv\\lib\\site-packages (from jsonschema>=2.6->nbformat->jupytext>=1.13.8->-r requirements.txt (line 31)) (0.25.1)\n",
      "Requirement already satisfied: pywin32>=300 in d:\\homework\\.venv\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat->jupytext>=1.13.8->-r requirements.txt (line 31)) (310)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch in d:\\homework\\.venv\\lib\\site-packages (2.7.0+cu126)\n",
      "Requirement already satisfied: numpy in d:\\homework\\.venv\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: transformers in d:\\homework\\.venv\\lib\\site-packages (4.52.4)\n",
      "Requirement already satisfied: datasets in d:\\homework\\.venv\\lib\\site-packages (3.6.0)\n",
      "Requirement already satisfied: wandb in d:\\homework\\.venv\\lib\\site-packages (0.20.1)\n",
      "Requirement already satisfied: tqdm in d:\\homework\\.venv\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: filelock in d:\\homework\\.venv\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in d:\\homework\\.venv\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in d:\\homework\\.venv\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in d:\\homework\\.venv\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in d:\\homework\\.venv\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in d:\\homework\\.venv\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in d:\\homework\\.venv\\lib\\site-packages (from torch) (70.2.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in d:\\homework\\.venv\\lib\\site-packages (from transformers) (0.32.4)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\homework\\.venv\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\homework\\.venv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\homework\\.venv\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in d:\\homework\\.venv\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in d:\\homework\\.venv\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in d:\\homework\\.venv\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in d:\\homework\\.venv\\lib\\site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in d:\\homework\\.venv\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in d:\\homework\\.venv\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in d:\\homework\\.venv\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in d:\\homework\\.venv\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in d:\\homework\\.venv\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.11)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in d:\\homework\\.venv\\lib\\site-packages (from wandb) (8.2.1)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in d:\\homework\\.venv\\lib\\site-packages (from wandb) (3.1.44)\n",
      "Requirement already satisfied: platformdirs in d:\\homework\\.venv\\lib\\site-packages (from wandb) (4.3.8)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in d:\\homework\\.venv\\lib\\site-packages (from wandb) (5.29.5)\n",
      "Requirement already satisfied: psutil>=5.0.0 in d:\\homework\\.venv\\lib\\site-packages (from wandb) (7.0.0)\n",
      "Requirement already satisfied: pydantic<3 in d:\\homework\\.venv\\lib\\site-packages (from wandb) (2.11.5)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in d:\\homework\\.venv\\lib\\site-packages (from wandb) (2.29.1)\n",
      "Requirement already satisfied: setproctitle in d:\\homework\\.venv\\lib\\site-packages (from wandb) (1.3.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\homework\\.venv\\lib\\site-packages (from pydantic<3->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in d:\\homework\\.venv\\lib\\site-packages (from pydantic<3->wandb) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in d:\\homework\\.venv\\lib\\site-packages (from pydantic<3->wandb) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\homework\\.venv\\lib\\site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\homework\\.venv\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\homework\\.venv\\lib\\site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\homework\\.venv\\lib\\site-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: colorama in d:\\homework\\.venv\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in d:\\homework\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\homework\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\homework\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\homework\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\homework\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\homework\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\homework\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in d:\\homework\\.venv\\lib\\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in d:\\homework\\.venv\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\homework\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\homework\\.venv\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\homework\\.venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\homework\\.venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\homework\\.venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\homework\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install -r requirements.txt\n",
    "\n",
    "# Additional packages for nanoGPT\n",
    "%pip install torch numpy transformers datasets wandb tqdm\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('models', exist_ok=True)\n",
    "## 1. Exploring the Synthetic Mention Corpora\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ace664a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Synthetic Mention Corpora from data/synthetic_mentions/SYNTHETIC_MENTIONS.csv\n"
     ]
    }
   ],
   "source": [
    "# Let's explore the Synthetic Mention Corpora for Disease Entity Recognition\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "os.makedirs('data/synthetic_mentions', exist_ok=True)\n",
    "\n",
    "# Function to download the dataset\n",
    "def download_synthetic_mentions():\n",
    "    \"\"\"\n",
    "    Download the Synthetic Mention Corpora for Disease Entity Recognition\n",
    "    \n",
    "    Note: You need to manually download this dataset from PhysioNet:\n",
    "    https://physionet.org/content/synthetic-mention-corpora/\n",
    "    \n",
    "    After downloading, place the files in the data/synthetic_mentions directory\n",
    "    \"\"\"\n",
    "    mentions_data_path = 'data/synthetic_mentions/SYNTHETIC_MENTIONS.csv'\n",
    "    \n",
    "    if os.path.exists(mentions_data_path):\n",
    "        print(f\"Loading Synthetic Mention Corpora from {mentions_data_path}\")\n",
    "        data = pd.read_csv(mentions_data_path)\n",
    "        # print(data.head())\n",
    "        return data\n",
    "    else:\n",
    "        print(f\"Synthetic Mention Corpora not found at {mentions_data_path}\")\n",
    "        print(\"Please download the dataset from PhysioNet:\")\n",
    "        print(\"https://physionet.org/content/synthetic-mention-corpora/\")\n",
    "        print(\"After downloading, place the files in the data/synthetic_mentions directory\")\n",
    "        return None\n",
    "\n",
    "# Try to load the dataset\n",
    "mentions_data = download_synthetic_mentions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2c13fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 96202788\n",
      "Total words: 14460515\n",
      "Total lines: 128963\n",
      "\n",
      "First few lines:\n",
      "1:  sig: one (1) tablet po qd. disp:*30 tablet(s)* refills:*0*  discharge disposition: home with service  discharge diagnosis: 1.  <1CUI> stable multiple myeloma or plasma cell leukemia </1CUI>  2. diabetes mellitus type ii 3. hypertension 4. hypercholesterolemia 5. hyperlipidemia 6. gout  discharge condition: good  discharge instructions: you will need to have blood drawn in the morning every two weeks.  you will need to return to [**hospital 2062**] for a follow up appointment with [**last name (stitle) **] in two weeks.  please call [**telephone/fax (2) 2942**] if you have any questions or concerns.  you should be taking your blood pressure medication as prescribed.   you should be taking your blood thinner medication as prescribed.   you should be taking your cholesterol medication as prescribed.   you should be taking\n",
      "2:  4.  pml. 5.   <1CUI> favorable hodgkin lymphoma </1CUI> . 6.  cmv.  social history:  she is a former smoker, who quit in [**2010-03-04**].  she works as a nurse and has two children.   family history:  non-contributory.  physical exam:  vs: t 99.5, bp 120/80, rr 18, hr 76, o2 sats 98%/ra.  general:  well-appearing woman in no acute distress.  heent:  perrla, anicteric sclerae,  mmm, no mass, no lesions, no lad.   neck:  supple, no lymphadenopathy, no thyromegaly.   chest:  lungs cta, no rales or rhonchi.   card:  rrr, s1, s2, no murmurs.   abd:  soft, nontender, nondist\n",
      "3:   <1CUI> t waves biphasic </1CUI> .   p waves: normal.   qt (qtc): 365.   cxr:  [**2012-09-10**]:  no evidence of pneumothorax.   ct abdomen:  impression:  no evidence of bowel obstruction or hematoma.  no evidence of active bleeding.   ct head:  impression:  no evidence of acute intracranial hemorrhage.   ekg [**2012-09-10**]:  t waves biphasic.   catheterization:  a 7 french j-tip catheter was placed in the right femoral artery.   conclusion:  1.  the patient was stabilized with vasopressors and was transferred to the [**doctor last name 60**] for further management.  2.  the patient was transferred to the [**doctor last name 60**] for further management of the bleed.  3.  the patient was stabilized\n",
      "4:  she was started on hydroxyurea, cyclosporine, and steroids for cytoreduction.   she was transferred to the [**hospital unit name 40**] for further management.  past medical history: 1.  chronic myelogenous leukemia  2.   <1CUI> flt3 internal tandem duplication </1CUI>   3.  essential hypertension  4.  hyperlipidemia  5.  gerd   social history: she is a non-smoker, drinks occasionally, and denies illicit drug use.  family history: non-contributory.  physical exam: on admission, the patient was afebrile, tachycardic, and tachypneic.  she had a left-sided chest tubes.  she had a palpable spleen, liver, and lymphadenopathy.   pertinent results:   [**2015-05-27**] 04:10am blood wbc-2.4 rbc-2.64* hgb-7.4* h\n",
      "5:  6.  cytogenic abnormalities: 10/10 metaphases examined.  7.   <1CUI> flt3-tkd mutation </1CUI> : 10/10 metaphases examined.  8.  mll (ki-67): 100% of metaphases examined.  9.  cdkn2a (p16): 100% of metaphases examined.  10.  tp53: 100% of metaphases examined.  11.  cdkn1a (p21): 100% of metaphases examined.  12.  histone h3 family 31 (h3f3a): 100% of metaphases examined.  13.  histone h3 family 31 (h3f3a): 100% of metaphases examined.  14.  mll (histone h4): 100% of metaphases examined.  15.  calr\n",
      "\n",
      "The extracted text seems suitable for training.\n",
      "Saved combined text to data/processed/mentions_text.txt\n"
     ]
    }
   ],
   "source": [
    "# If the dataset is loaded successfully, convert to text for training\n",
    "if mentions_data is not None:\n",
    "    # Extract 'matched_output' and combine into a single text\n",
    "    mentions_text = \"\\n\".join(mentions_data[\"matched_output\"].astype(str).tolist())\n",
    "    \n",
    "    # Print some statistics\n",
    "    print(f\"Total characters: {len(mentions_text)}\")\n",
    "    print(f\"Total words: {len(mentions_text.split())}\")\n",
    "    print(f\"Total lines: {len(mentions_text.splitlines())}\")\n",
    "    \n",
    "    # Print the first few lines\n",
    "    print(\"\\nFirst few lines:\")\n",
    "    for i, line in enumerate(mentions_text.splitlines()[:5]):\n",
    "        print(f\"{i+1}: {line}\")\n",
    "    \n",
    "    # Check if the data is suitable for training\n",
    "    if len(mentions_text) < 100000:  # Less than 100KB\n",
    "        print(\"\\nWarning: The extracted text might be too small for effective training.\")\n",
    "        print(\"Consider using more entries from the dataset.\")\n",
    "    else:\n",
    "        print(\"\\nThe extracted text seems suitable for training.\")\n",
    "    \n",
    "    # Save the combined text for training\n",
    "    with open('data/processed/mentions_text.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write(mentions_text)\n",
    "    print(\"Saved combined text to data/processed/mentions_text.txt\")\n",
    "else:\n",
    "    # Fallback to open_db.txt if the dataset is not available\n",
    "    print(\"Falling back to open_db.txt for training\")\n",
    "    \n",
    "    def read_open_db():\n",
    "        \"\"\"Read the open database text file\"\"\"\n",
    "        with open('open_db.txt', 'r') as f:\n",
    "            text = f.read()\n",
    "        return text\n",
    "    \n",
    "    # Read the open database text\n",
    "    mentions_text = read_open_db()\n",
    "    \n",
    "    # Print some statistics\n",
    "    print(f\"Total characters: {len(mentions_text)}\")\n",
    "    print(f\"Total words: {len(mentions_text.split())}\")\n",
    "    print(f\"Total lines: {len(mentions_text.splitlines())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "703e1190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 75 characters\n",
      "Encoded text length: 96202788 tokens\n",
      "Tokenizer saved to data/processed/char_tokenizer.json\n",
      "Train data size: 86582509 tokens\n",
      "Validation data size: 9620279 tokens\n",
      "Processed data saved to data/processed/\n",
      "Number of training batches: 338212\n",
      "Number of validation batches: 37579\n",
      "Input shape: torch.Size([256, 256])\n",
      "Target shape: torch.Size([256, 256])\n",
      "Sample input:  management.   physical examination:  the patient ...\n",
      "Sample target: management.   physical examination:  the patient w...\n"
     ]
    }
   ],
   "source": [
    "## 2. Data Preprocessing\n",
    "# Let's preprocess the text data for training\n",
    "\n",
    "class CharacterTokenizer:\n",
    "    \"\"\"Simple character-level tokenizer\"\"\"\n",
    "    \n",
    "    def __init__(self, text):\n",
    "        \"\"\"Initialize the tokenizer with the training text\"\"\"\n",
    "        self.chars = sorted(list(set(text)))\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.stoi = {ch: i for i, ch in enumerate(self.chars)}\n",
    "        self.itos = {i: ch for i, ch in enumerate(self.chars)}\n",
    "        \n",
    "        print(f\"Vocabulary size: {self.vocab_size} characters\")\n",
    "    \n",
    "    def encode(self, text):\n",
    "        \"\"\"Encode text to token IDs\"\"\"\n",
    "        return [self.stoi[ch] for ch in text]\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        \"\"\"Decode token IDs to text\"\"\"\n",
    "        return ''.join([self.itos[id] for id in ids])\n",
    "    \n",
    "    def save(self, path):\n",
    "        \"\"\"Save the tokenizer to a file\"\"\"\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump({\n",
    "                'chars': self.chars,\n",
    "                'vocab_size': self.vocab_size,\n",
    "                'stoi': self.stoi,\n",
    "                'itos': {str(k): v for k, v in self.itos.items()}  # Convert keys to strings for JSON\n",
    "            }, f)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        \"\"\"Load a tokenizer from a file\"\"\"\n",
    "        with open(path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        tokenizer = cls.__new__(cls)\n",
    "        tokenizer.chars = data['chars']\n",
    "        tokenizer.vocab_size = data['vocab_size']\n",
    "        tokenizer.stoi = data['stoi']\n",
    "        tokenizer.itos = {int(k): v for k, v in data['itos'].items()}  # Convert keys back to integers\n",
    "        \n",
    "        return tokenizer\n",
    "\n",
    "# Create a tokenizer from the mentions text\n",
    "tokenizer = CharacterTokenizer(mentions_text)\n",
    "\n",
    "# Encode the entire text\n",
    "encoded_text = tokenizer.encode(mentions_text)\n",
    "print(f\"Encoded text length: {len(encoded_text)} tokens\")\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save('data/processed/char_tokenizer.json')\n",
    "print(\"Tokenizer saved to data/processed/char_tokenizer.json\")\n",
    "\n",
    "# Split the data into train and validation sets (90% train, 10% validation)\n",
    "train_size = int(0.9 * len(encoded_text))\n",
    "train_data = encoded_text[:train_size]\n",
    "val_data = encoded_text[train_size:]\n",
    "\n",
    "print(f\"Train data size: {len(train_data)} tokens\")\n",
    "print(f\"Validation data size: {len(val_data)} tokens\")\n",
    "\n",
    "# Save the processed data\n",
    "np.save('data/processed/train_data.npy', np.array(train_data, dtype=np.int16))\n",
    "np.save('data/processed/val_data.npy', np.array(val_data, dtype=np.int16))\n",
    "print(\"Processed data saved to data/processed/\")\n",
    "\n",
    "# Create a dataset class for training\n",
    "class TextDataset(Dataset):\n",
    "    \"\"\"Dataset for training a language model\"\"\"\n",
    "    \n",
    "    def __init__(self, data, context_length=256):\n",
    "        \"\"\"\n",
    "        Initialize the dataset\n",
    "        \n",
    "        Args:\n",
    "            data: List of token IDs\n",
    "            context_length: Context length for prediction\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.context_length = context_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of possible contexts\"\"\"\n",
    "        return len(self.data) - self.context_length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Get a context and target pair\"\"\"\n",
    "        context = self.data[idx:idx+self.context_length]\n",
    "        target = self.data[idx+1:idx+self.context_length+1]\n",
    "        return torch.tensor(context, dtype=torch.long), torch.tensor(target, dtype=torch.long)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "context_length = 256\n",
    "batch_size = 256\n",
    "\n",
    "train_dataset = TextDataset(train_data, context_length)\n",
    "val_dataset = TextDataset(val_data, context_length)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "num_workers = min(os.cpu_count() or 1, 4)\n",
    "\n",
    "print(f\"Number of training batches: {len(train_loader)}\")\n",
    "print(f\"Number of validation batches: {len(val_loader)}\")\n",
    "\n",
    "# Check a sample batch\n",
    "x, y = next(iter(train_loader))\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"Sample input: {tokenizer.decode(x[0].tolist()[:50])}...\")\n",
    "print(f\"Sample target: {tokenizer.decode(y[0].tolist()[:50])}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df37513b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NanoGPT model with 0.04M parameters\n",
      "Using device: cuda\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "# Let's implement a small GPT model (nanoGPT)\n",
    "os.makedirs('results/part_4', exist_ok=True)\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "## 3. Implementing the nanoGPT Model\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-head self-attention module\"\"\"\n",
    "    \n",
    "    def __init__(self, n_embd, n_head, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initialize the multi-head attention module\n",
    "        \n",
    "        Args:\n",
    "            n_embd: Embedding dimension\n",
    "            n_head: Number of attention heads\n",
    "            dropout: Dropout probability\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert n_embd % n_head == 0, \"Embedding dimension must be divisible by number of heads\"\n",
    "        \n",
    "        self.n_embd = n_embd\n",
    "        self.n_head = n_head\n",
    "        self.head_dim = n_embd // n_head\n",
    "        \n",
    "        # Key, query, value projections\n",
    "        self.query = nn.Linear(n_embd, n_embd)\n",
    "        self.key = nn.Linear(n_embd, n_embd)\n",
    "        self.value = nn.Linear(n_embd, n_embd)\n",
    "        \n",
    "        # Output projection\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        \n",
    "        # Regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Causal mask to ensure that attention is only applied to the left\n",
    "        self.register_buffer(\n",
    "            \"mask\", \n",
    "            torch.tril(torch.ones(context_length, context_length))\n",
    "            .view(1, 1, context_length, context_length)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        batch_size, seq_len, n_embd = x.size()\n",
    "        \n",
    "        # Calculate query, key, values\n",
    "        q = self.query(x).view(batch_size, seq_len, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        k = self.key(x).view(batch_size, seq_len, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        v = self.value(x).view(batch_size, seq_len, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        # (batch_size, n_head, seq_len, head_dim) x (batch_size, n_head, head_dim, seq_len)\n",
    "        # -> (batch_size, n_head, seq_len, seq_len)\n",
    "        attn = (q @ k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        \n",
    "        # Apply causal mask\n",
    "        attn = attn.masked_fill(self.mask[:, :, :seq_len, :seq_len] == 0, float('-inf'))\n",
    "        \n",
    "        # Softmax and dropout\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        # (batch_size, n_head, seq_len, seq_len) x (batch_size, n_head, seq_len, head_dim)\n",
    "        # -> (batch_size, n_head, seq_len, head_dim)\n",
    "        out = attn @ v\n",
    "        \n",
    "        # Reshape and project\n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, n_embd)\n",
    "        out = self.proj(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Feed-forward network\"\"\"\n",
    "    \n",
    "    def __init__(self, n_embd, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initialize the feed-forward network\n",
    "        \n",
    "        Args:\n",
    "            n_embd: Embedding dimension\n",
    "            dropout: Dropout probability\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer block\"\"\"\n",
    "    \n",
    "    def __init__(self, n_embd, n_head, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initialize the transformer block\n",
    "        \n",
    "        Args:\n",
    "            n_embd: Embedding dimension\n",
    "            n_head: Number of attention heads\n",
    "            dropout: Dropout probability\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.attn = MultiHeadAttention(n_embd, n_head, dropout)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        self.ff = FeedForward(n_embd, dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class NanoGPT(nn.Module):\n",
    "    \"\"\"Small GPT model\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, n_embd=128, n_head=4, n_layer=4, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initialize the nanoGPT model\n",
    "        \n",
    "        Args:\n",
    "            vocab_size: Size of the vocabulary\n",
    "            n_embd: Embedding dimension\n",
    "            n_head: Number of attention heads\n",
    "            n_layer: Number of transformer blocks\n",
    "            dropout: Dropout probability\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_embd = n_embd\n",
    "        self.n_head = n_head\n",
    "        self.n_layer = n_layer\n",
    "        \n",
    "        # Token embedding\n",
    "        self.token_embedding = nn.Embedding(vocab_size, n_embd)\n",
    "        \n",
    "        # Position embedding\n",
    "        self.position_embedding = nn.Embedding(context_length, n_embd)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(n_embd, n_head, dropout) for _ in range(n_layer)\n",
    "        ])\n",
    "        \n",
    "        # Final layer normalization\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        \n",
    "        # Output head\n",
    "        self.head = nn.Linear(n_embd, vocab_size, bias=False)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "        # Print model size\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        print(f\"NanoGPT model with {n_params/1e6:.2f}M parameters\")\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Initialize weights\"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def forward(self, idx):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        batch_size, seq_len = idx.size()\n",
    "        \n",
    "        # Get token and position embeddings\n",
    "        token_emb = self.token_embedding(idx)\n",
    "        pos = torch.arange(0, seq_len, dtype=torch.long, device=idx.device).unsqueeze(0)\n",
    "        pos_emb = self.position_embedding(pos)\n",
    "        \n",
    "        # Combine embeddings\n",
    "        x = token_emb + pos_emb\n",
    "        \n",
    "        # Apply transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        # Apply final layer norm\n",
    "        x = self.ln_f(x)\n",
    "        \n",
    "        # Apply output head\n",
    "        logits = self.head(x)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0):\n",
    "        \"\"\"\n",
    "        Generate text from the model\n",
    "        \n",
    "        Args:\n",
    "            idx: Starting token IDs (batch_size, seq_len)\n",
    "            max_new_tokens: Maximum number of new tokens to generate\n",
    "            temperature: Temperature for sampling (higher = more random)\n",
    "            \n",
    "        Returns:\n",
    "            Generated token IDs\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_new_tokens):\n",
    "                # Crop context if it's too long\n",
    "                idx_cond = idx if idx.size(1) <= context_length else idx[:, -context_length:]\n",
    "                \n",
    "                # Get predictions\n",
    "                logits = self(idx_cond)\n",
    "                \n",
    "                # Focus on the last token\n",
    "                logits = logits[:, -1, :] / temperature\n",
    "                \n",
    "                # Apply softmax to get probabilities\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                \n",
    "                # Sample from the distribution\n",
    "                idx_next = torch.multinomial(probs, num_samples=1)\n",
    "                \n",
    "                # Append to the sequence\n",
    "                idx = torch.cat((idx, idx_next), dim=1)\n",
    "        \n",
    "        return idx\n",
    "\n",
    "model = NanoGPT(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    n_embd=32,\n",
    "    n_head=2,\n",
    "    n_layer=2,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = model.to(device)\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50818eb4",
   "metadata": {},
   "source": [
    "## 4. Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d03be59d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zhengwei\\AppData\\Local\\Temp\\ipykernel_27468\\2778986290.py:29: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "Epoch 1/1 [Train]:   0%|          | 0/338212 [00:00<?, ?it/s]C:\\Users\\Zhengwei\\AppData\\Local\\Temp\\ipykernel_27468\\2778986290.py:53: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 1/1 [Train]:   7%|▋         | 23497/338212 [10:16<2:17:39, 38.10it/s, loss=1.54]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 156\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[32m    155\u001b[39m epochs = \u001b[32m1\u001b[39m  \u001b[38;5;66;03m# Adjust based on your computational resources\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m history = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[38;5;66;03m# Plot training history\u001b[39;00m\n\u001b[32m    159\u001b[39m plt.figure(figsize=(\u001b[32m12\u001b[39m, \u001b[32m5\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 72\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, epochs, lr)\u001b[39m\n\u001b[32m     69\u001b[39m torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=\u001b[32m1.0\u001b[39m)\n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m# Update weights\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m \u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     73\u001b[39m scaler.update()\n\u001b[32m     75\u001b[39m \u001b[38;5;66;03m# Update statistics\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\homework\\.venv\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:461\u001b[39m, in \u001b[36mGradScaler.step\u001b[39m\u001b[34m(self, optimizer, *args, **kwargs)\u001b[39m\n\u001b[32m    455\u001b[39m     \u001b[38;5;28mself\u001b[39m.unscale_(optimizer)\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[32m    458\u001b[39m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mfound_inf_per_device\u001b[39m\u001b[33m\"\u001b[39m]) > \u001b[32m0\u001b[39m\n\u001b[32m    459\u001b[39m ), \u001b[33m\"\u001b[39m\u001b[33mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m461\u001b[39m retval = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_opt_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    463\u001b[39m optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mstage\u001b[39m\u001b[33m\"\u001b[39m] = OptState.STEPPED\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\homework\\.venv\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:355\u001b[39m, in \u001b[36mGradScaler._maybe_opt_step\u001b[39m\u001b[34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[39m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_maybe_opt_step\u001b[39m(\n\u001b[32m    348\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    349\u001b[39m     optimizer: torch.optim.Optimizer,\n\u001b[32m   (...)\u001b[39m\u001b[32m    352\u001b[39m     **kwargs: Any,\n\u001b[32m    353\u001b[39m ) -> Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[32m    354\u001b[39m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf_per_device\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    356\u001b[39m         retval = optimizer.step(*args, **kwargs)\n\u001b[32m    357\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\homework\\.venv\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:355\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_maybe_opt_step\u001b[39m(\n\u001b[32m    348\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    349\u001b[39m     optimizer: torch.optim.Optimizer,\n\u001b[32m   (...)\u001b[39m\u001b[32m    352\u001b[39m     **kwargs: Any,\n\u001b[32m    353\u001b[39m ) -> Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[32m    354\u001b[39m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mfound_inf_per_device\u001b[39m\u001b[33m\"\u001b[39m].values()):\n\u001b[32m    356\u001b[39m         retval = optimizer.step(*args, **kwargs)\n\u001b[32m    357\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Let's train the nanoGPT model\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs=10, lr=3e-4):\n",
    "    \"\"\"\n",
    "    Train the model\n",
    "    \n",
    "    Args:\n",
    "        model: The model to train\n",
    "        train_loader: Training data loader\n",
    "        val_loader: Validation data loader\n",
    "        epochs: Number of epochs to train for\n",
    "        lr: Learning rate\n",
    "        \n",
    "    Returns:\n",
    "        Training history\n",
    "    \"\"\"\n",
    "    # Initialize optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Initialize learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    \n",
    "    # Initialize loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # For Mixed Precision Training (AMP)\n",
    "    # This automatically casts operations to float16 where appropriate,\n",
    "    # and scales gradients to prevent underflow.\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    \n",
    "    # Initialize training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'train_perplexity': [],\n",
    "        'val_perplexity': []\n",
    "    }\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_batches = 0\n",
    "        \n",
    "        # Progress bar for training\n",
    "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\")\n",
    "        for x, y in train_pbar:\n",
    "            # Move data to device\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            # Forward pass with autocast for mixed precision\n",
    "            with torch.cuda.amp.autocast():\n",
    "                logits = model(x)\n",
    "                \n",
    "                # Reshape for loss calculation\n",
    "                logits = logits.view(-1, tokenizer.vocab_size)\n",
    "                y = y.view(-1)\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = criterion(logits, y)\n",
    "            \n",
    "            # Backward pass with gradient scaler\n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            # Gradient clipping (apply before scaler.step())\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            # Update weights\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            # Update statistics\n",
    "            train_loss += loss.item()\n",
    "            train_batches += 1\n",
    "            \n",
    "            # Update progress bar\n",
    "            train_pbar.set_postfix({'loss': train_loss / train_batches})\n",
    "        \n",
    "        # Calculate average training loss\n",
    "        avg_train_loss = train_loss / train_batches\n",
    "        train_perplexity = np.exp(avg_train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_batches = 0\n",
    "        \n",
    "        # Progress bar for validation\n",
    "        val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Val]\")\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_pbar:\n",
    "                # Move data to device\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                \n",
    "                # Forward pass with autocast for mixed precision in validation too\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    logits = model(x)\n",
    "                    \n",
    "                    # Reshape for loss calculation\n",
    "                    logits = logits.view(-1, tokenizer.vocab_size)\n",
    "                    y = y.view(-1)\n",
    "                    \n",
    "                    # Calculate loss\n",
    "                    loss = criterion(logits, y)\n",
    "                \n",
    "                # Update statistics\n",
    "                val_loss += loss.item()\n",
    "                val_batches += 1\n",
    "                \n",
    "                # Update progress bar\n",
    "                val_pbar.set_postfix({'loss': val_loss / val_batches})\n",
    "        \n",
    "        # Calculate average validation loss\n",
    "        avg_val_loss = val_loss / val_batches\n",
    "        val_perplexity = np.exp(avg_val_loss)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "              f\"Train Perplexity: {train_perplexity:.4f}, \"\n",
    "              f\"Val Loss: {avg_val_loss:.4f}, \"\n",
    "              f\"Val Perplexity: {val_perplexity:.4f}\")\n",
    "        \n",
    "        # Update history\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['train_perplexity'].append(train_perplexity)\n",
    "        history['val_perplexity'].append(val_perplexity)\n",
    "        \n",
    "        # Save checkpoint\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'train_loss': avg_train_loss,\n",
    "            'val_loss': avg_val_loss,\n",
    "            'scaler_state_dict': scaler.state_dict()\n",
    "        }, f'models/nanogpt_checkpoint_epoch_{epoch+1}.pt')\n",
    "    \n",
    "    # Save final model\n",
    "    torch.save(model.state_dict(), 'models/nanogpt.pt')\n",
    "    print(\"Model saved to models/nanogpt.pt\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "# Train the model\n",
    "epochs = 1  # Adjust based on your computational resources\n",
    "history = train_model(model, train_loader, val_loader, epochs=epochs)\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history['train_loss'], label='Train')\n",
    "plt.plot(history['val_loss'], label='Validation')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot perplexity\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history['train_perplexity'], label='Train')\n",
    "plt.plot(history['val_perplexity'], label='Validation')\n",
    "plt.title('Perplexity')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Perplexity')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/part_4/training_history.png')\n",
    "plt.show()\n",
    "\n",
    "# Save training metrics\n",
    "with open('results/part_4/training_metrics.txt', 'w') as f:\n",
    "    f.write(\"# NanoGPT Training Metrics\\n\\n\")\n",
    "    f.write(\"## Model Configuration\\n\")\n",
    "    f.write(f\"Vocabulary Size: {tokenizer.vocab_size}\\n\")\n",
    "    f.write(f\"Embedding Dimension: {model.n_embd}\\n\")\n",
    "## 5. Generating Text and Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2eede1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's generate text from the trained model and evaluate it\n",
    "\n",
    "def generate_text(model, tokenizer, prompt, max_new_tokens=100, temperature=0.8):\n",
    "    \"\"\"\n",
    "    Generate text from the model\n",
    "    \n",
    "    Args:\n",
    "        model: The trained model\n",
    "        tokenizer: The tokenizer\n",
    "        prompt: The prompt text\n",
    "        max_new_tokens: Maximum number of new tokens to generate\n",
    "        temperature: Temperature for sampling (higher = more random)\n",
    "        \n",
    "    Returns:\n",
    "        Generated text\n",
    "    \"\"\"\n",
    "    # Encode the prompt\n",
    "    encoded_prompt = tokenizer.encode(prompt)\n",
    "    \n",
    "    # Convert to tensor and add batch dimension\n",
    "    x = torch.tensor([encoded_prompt], dtype=torch.long).to(device)\n",
    "    \n",
    "    # Generate text\n",
    "    output = model.generate(x, max_new_tokens=max_new_tokens, temperature=temperature)\n",
    "    \n",
    "    # Decode the output\n",
    "    generated_text = tokenizer.decode(output[0].tolist())\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "# Load the trained model\n",
    "model.load_state_dict(torch.load('models/nanogpt.pt'))\n",
    "model.eval()\n",
    "\n",
    "# Generate text with different prompts\n",
    "prompts = [\n",
    "    \"Diabetes is a chronic condition that\",\n",
    "    \"The symptoms of heart disease include\",\n",
    "    \"To prevent respiratory infections,\",\n",
    "    \"The treatment for hypertension typically involves\",\n",
    "    \"Mental health disorders are characterized by\"\n",
    "]\n",
    "\n",
    "# Generate and print text for each prompt\n",
    "print(\"Generated Text Samples:\")\n",
    "for i, prompt in enumerate(prompts):\n",
    "    print(f\"\\nPrompt {i+1}: {prompt}\")\n",
    "    generated_text = generate_text(model, tokenizer, prompt, max_new_tokens=100)\n",
    "    print(f\"Generated: {generated_text}\")\n",
    "\n",
    "# Compare with larger pre-trained models\n",
    "try:\n",
    "    from transformers import pipeline\n",
    "    \n",
    "    # Load a pre-trained model\n",
    "    generator = pipeline('text-generation', model='gpt2')\n",
    "    \n",
    "    print(\"\\n\\nComparison with GPT-2:\")\n",
    "    for i, prompt in enumerate(prompts[:2]):  # Just try a couple of prompts\n",
    "        print(f\"\\nPrompt: {prompt}\")\n",
    "        \n",
    "        # Generate with our nanoGPT\n",
    "        nano_text = generate_text(model, tokenizer, prompt, max_new_tokens=50)\n",
    "        print(f\"NanoGPT: {nano_text}\")\n",
    "        \n",
    "        # Generate with GPT-2\n",
    "        gpt2_text = generator(prompt, max_length=50, num_return_sequences=1)[0]['generated_text']\n",
    "        print(f\"GPT-2: {gpt2_text}\")\n",
    "except:\n",
    "    print(\"\\nSkipping comparison with pre-trained models (requires internet connection)\")\n",
    "\n",
    "# Evaluate the quality of generated text\n",
    "def evaluate_generated_text(generated_samples):\n",
    "    \"\"\"\n",
    "    Evaluate the quality of generated text\n",
    "    \n",
    "    Args:\n",
    "        generated_samples: List of generated text samples\n",
    "        \n",
    "    Returns:\n",
    "        Evaluation metrics\n",
    "    \"\"\"\n",
    "    # Simple metrics for text quality\n",
    "    metrics = {\n",
    "        'avg_length': 0,\n",
    "        'unique_words': 0,\n",
    "        'repetition_rate': 0\n",
    "    }\n",
    "    \n",
    "    total_length = 0\n",
    "    total_unique_words = 0\n",
    "    total_repetition_rate = 0\n",
    "    \n",
    "    for text in generated_samples:\n",
    "        # Calculate length\n",
    "        words = text.split()\n",
    "        length = len(words)\n",
    "        total_length += length\n",
    "        \n",
    "        # Calculate unique words\n",
    "        unique_words = len(set(words))\n",
    "        total_unique_words += unique_words\n",
    "        \n",
    "        # Calculate repetition rate (lower is better)\n",
    "        if length > 0:\n",
    "            repetition_rate = 1 - (unique_words / length)\n",
    "        else:\n",
    "            repetition_rate = 0\n",
    "        total_repetition_rate += repetition_rate\n",
    "    \n",
    "    # Calculate averages\n",
    "    n_samples = len(generated_samples)\n",
    "    if n_samples > 0:\n",
    "        metrics['avg_length'] = total_length / n_samples\n",
    "        metrics['unique_words'] = total_unique_words / n_samples\n",
    "        metrics['repetition_rate'] = total_repetition_rate / n_samples\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Generate a larger set of samples for evaluation\n",
    "evaluation_prompts = [\n",
    "    \"The patient presented with\",\n",
    "    \"Common side effects include\",\n",
    "    \"The diagnosis was confirmed by\",\n",
    "    \"Treatment options for this condition\",\n",
    "    \"The prognosis for patients with\"\n",
    "]\n",
    "\n",
    "generated_samples = []\n",
    "for prompt in evaluation_prompts:\n",
    "    for temp in [0.7, 0.8, 0.9]:  # Try different temperatures\n",
    "        generated_text = generate_text(model, tokenizer, prompt, max_new_tokens=100, temperature=temp)\n",
    "        generated_samples.append(generated_text)\n",
    "\n",
    "# Evaluate the generated samples\n",
    "evaluation_metrics = evaluate_generated_text(generated_samples)\n",
    "print(\"\\nGenerated Text Evaluation:\")\n",
    "for metric, value in evaluation_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Save evaluation results\n",
    "with open('results/part_4/generation_evaluation.txt', 'w') as f:\n",
    "    f.write(\"# NanoGPT Text Generation Evaluation\\n\\n\")\n",
    "    \n",
    "    f.write(\"## Evaluation Metrics\\n\")\n",
    "    for metric, value in evaluation_metrics.items():\n",
    "        f.write(f\"{metric}: {value:.4f}\\n\")\n",
    "    \n",
    "    f.write(\"\\n## Generated Samples\\n\")\n",
    "    for i, (prompt, sample) in enumerate(zip(evaluation_prompts * 3, generated_samples)):\n",
    "        f.write(f\"\\nSample {i+1}:\\n\")\n",
    "        f.write(f\"Prompt: {prompt}\\n\")\n",
    "        f.write(f\"Generated: {sample}\\n\")\n",
    "        f.write(\"-\" * 50 + \"\\n\")\n",
    "\n",
    "print(\"Evaluation results saved to results/part_4/generation_evaluation.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aee9689",
   "metadata": {},
   "source": [
    "## Progress Checkpoints\n",
    "\n",
    "1. **Data Exploration**:\n",
    "   - [ ] Download and analyze the Synthetic Mention Corpora\n",
    "   - [ ] Extract disease mentions and contexts\n",
    "   - [ ] Determine if it's suitable for training\n",
    "   - [ ] Prepare the text data for model training\n",
    "\n",
    "2. **Data Preprocessing**:\n",
    "   - [ ] Create a character-level tokenizer\n",
    "   - [ ] Encode the text data\n",
    "   - [ ] Split into train and validation sets\n",
    "\n",
    "3. **Model Implementation**:\n",
    "   - [ ] Implement the nanoGPT architecture\n",
    "   - [ ] Configure model size based on available resources\n",
    "   - [ ] Verify model structure and parameter count\n",
    "\n",
    "4. **Training**:\n",
    "   - [ ] Train the model with appropriate hyperparameters\n",
    "   - [ ] Monitor training progress\n",
    "   - [ ] Save checkpoints and final model\n",
    "\n",
    "5. **Evaluation**:\n",
    "   - [ ] Generate text with different prompts\n",
    "   - [ ] Compare with larger pre-trained models\n",
    "   - [ ] Evaluate text quality metrics\n",
    "   - [ ] Save evaluation results\n",
    "    f.write(f\"Number of Heads: {model.n_head}\\n\")\n",
    "    f.write(f\"Number of Layers: {model.n_layer}\\n\")\n",
    "    f.write(f\"Context Length: {context_length}\\n\")\n",
    "    f.write(f\"Batch Size: {batch_size}\\n\")\n",
    "    f.write(f\"Epochs: {epochs}\\n\\n\")\n",
    "    \n",
    "    f.write(\"## Training Results\\n\")\n",
    "    f.write(f\"Final Train Loss: {history['train_loss'][-1]:.4f}\\n\")\n",
    "    f.write(f\"Final Validation Loss: {history['val_loss'][-1]:.4f}\\n\")\n",
    "    f.write(f\"Final Train Perplexity: {history['train_perplexity'][-1]:.4f}\\n\")\n",
    "    f.write(f\"Final Validation Perplexity: {history['val_perplexity'][-1]:.4f}\\n\\n\")\n",
    "    \n",
    "    f.write(\"## Epoch-by-Epoch Metrics\\n\")\n",
    "    for i in range(epochs):\n",
    "        f.write(f\"Epoch {i+1}:\\n\")\n",
    "        f.write(f\"  Train Loss: {history['train_loss'][i]:.4f}\\n\")\n",
    "        f.write(f\"  Val Loss: {history['val_loss'][i]:.4f}\\n\")\n",
    "        f.write(f\"  Train Perplexity: {history['train_perplexity'][i]:.4f}\\n\")\n",
    "        f.write(f\"  Val Perplexity: {history['val_perplexity'][i]:.4f}\\n\")\n",
    "\n",
    "print(\"Training metrics saved to results/part_4/training_metrics.txt\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
