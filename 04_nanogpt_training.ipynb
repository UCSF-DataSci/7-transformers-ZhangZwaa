{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bd44fc0",
   "metadata": {},
   "source": [
    "# Part 4: Optional - nanoGPT Training on Healthcare Text Data\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this optional part, you'll train a small GPT model (nanoGPT) on healthcare text data. This will give you hands-on experience with training language models from scratch and understanding their capabilities and limitations. You'll use the Synthetic Mention Corpora for Disease Entity Recognition, which contains 128,000 disease mentions generated by an LLM.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Understand the architecture of small language models\n",
    "- Prepare text data for language model training\n",
    "- Train a nanoGPT model on domain-specific data\n",
    "- Evaluate model performance and generated text quality\n",
    "- Compare with larger pre-trained models\n",
    "\n",
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7549f155",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy>=1.20.0 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from -r requirements.txt (line 2)) (2.1.3)\n",
      "Requirement already satisfied: pandas>=1.3.0 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from -r requirements.txt (line 3)) (2.2.3)\n",
      "Requirement already satisfied: matplotlib>=3.4.0 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from -r requirements.txt (line 4)) (3.10.3)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from -r requirements.txt (line 5)) (0.13.2)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from -r requirements.txt (line 6)) (1.6.1)\n",
      "Requirement already satisfied: tqdm>=4.62.0 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from -r requirements.txt (line 7)) (4.67.1)\n",
      "Requirement already satisfied: tensorflow>=2.8.0 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from -r requirements.txt (line 10)) (2.19.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from -r requirements.txt (line 11)) (2.7.0)\n",
      "Requirement already satisfied: transformers>=4.18.0 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from -r requirements.txt (line 12)) (4.51.3)\n",
      "Requirement already satisfied: requests>=2.27.0 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from -r requirements.txt (line 15)) (2.32.3)\n",
      "Requirement already satisfied: huggingface_hub>=0.5.0 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from -r requirements.txt (line 16)) (0.31.2)\n",
      "Requirement already satisfied: accelerate>=0.12.0 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from -r requirements.txt (line 17)) (1.6.0)\n",
      "Requirement already satisfied: sentencepiece>=0.1.96 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from -r requirements.txt (line 18)) (0.2.0)\n",
      "Requirement already satisfied: tokenizers>=0.12.1 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from -r requirements.txt (line 19)) (0.21.1)\n",
      "Requirement already satisfied: datasets>=2.0.0 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from -r requirements.txt (line 22)) (3.6.0)\n",
      "Requirement already satisfied: regex>=2022.3.15 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from -r requirements.txt (line 23)) (2024.11.6)\n",
      "Requirement already satisfied: plotly>=5.6.0 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from -r requirements.txt (line 26)) (6.0.1)\n",
      "Requirement already satisfied: wandb>=0.12.0 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from -r requirements.txt (line 29)) (0.19.11)\n",
      "Requirement already satisfied: pytest>=7.0.0 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from -r requirements.txt (line 30)) (8.3.5)\n",
      "Requirement already satisfied: jupytext>=1.13.8 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from -r requirements.txt (line 31)) (1.17.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from pandas>=1.3.0->-r requirements.txt (line 3)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from pandas>=1.3.0->-r requirements.txt (line 3)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from pandas>=1.3.0->-r requirements.txt (line 3)) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from matplotlib>=3.4.0->-r requirements.txt (line 4)) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from matplotlib>=3.4.0->-r requirements.txt (line 4)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from matplotlib>=3.4.0->-r requirements.txt (line 4)) (4.58.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from matplotlib>=3.4.0->-r requirements.txt (line 4)) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from matplotlib>=3.4.0->-r requirements.txt (line 4)) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from matplotlib>=3.4.0->-r requirements.txt (line 4)) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from matplotlib>=3.4.0->-r requirements.txt (line 4)) (3.2.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from scikit-learn>=1.0.0->-r requirements.txt (line 6)) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from scikit-learn>=1.0.0->-r requirements.txt (line 6)) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from scikit-learn>=1.0.0->-r requirements.txt (line 6)) (3.6.0)\n",
      "Requirement already satisfied: colorama in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from tqdm>=4.62.0->-r requirements.txt (line 7)) (0.4.6)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from tensorflow>=2.8.0->-r requirements.txt (line 10)) (2.2.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from tensorflow>=2.8.0->-r requirements.txt (line 10)) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from tensorflow>=2.8.0->-r requirements.txt (line 10)) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from tensorflow>=2.8.0->-r requirements.txt (line 10)) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from tensorflow>=2.8.0->-r requirements.txt (line 10)) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from tensorflow>=2.8.0->-r requirements.txt (line 10)) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from tensorflow>=2.8.0->-r requirements.txt (line 10)) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from tensorflow>=2.8.0->-r requirements.txt (line 10)) (5.29.4)\n",
      "Requirement already satisfied: setuptools in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from tensorflow>=2.8.0->-r requirements.txt (line 10)) (80.4.0)\n",
      "Requirement already satisfied: six>=1.12.0 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from tensorflow>=2.8.0->-r requirements.txt (line 10)) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from tensorflow>=2.8.0->-r requirements.txt (line 10)) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from tensorflow>=2.8.0->-r requirements.txt (line 10)) (4.13.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from tensorflow>=2.8.0->-r requirements.txt (line 10)) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from tensorflow>=2.8.0->-r requirements.txt (line 10)) (1.71.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from tensorflow>=2.8.0->-r requirements.txt (line 10)) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from tensorflow>=2.8.0->-r requirements.txt (line 10)) (3.9.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from tensorflow>=2.8.0->-r requirements.txt (line 10)) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from tensorflow>=2.8.0->-r requirements.txt (line 10)) (0.5.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from requests>=2.27.0->-r requirements.txt (line 15)) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from requests>=2.27.0->-r requirements.txt (line 15)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from requests>=2.27.0->-r requirements.txt (line 15)) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from requests>=2.27.0->-r requirements.txt (line 15)) (2025.4.26)\n",
      "Requirement already satisfied: markdown>=2.6.8 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow>=2.8.0->-r requirements.txt (line 10)) (3.8)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow>=2.8.0->-r requirements.txt (line 10)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow>=2.8.0->-r requirements.txt (line 10)) (3.1.3)\n",
      "Requirement already satisfied: filelock in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from torch>=1.10.0->-r requirements.txt (line 11)) (3.18.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from torch>=1.10.0->-r requirements.txt (line 11)) (1.14.0)\n",
      "Requirement already satisfied: networkx in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from torch>=1.10.0->-r requirements.txt (line 11)) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from torch>=1.10.0->-r requirements.txt (line 11)) (3.1.6)\n",
      "Requirement already satisfied: fsspec in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from torch>=1.10.0->-r requirements.txt (line 11)) (2025.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from transformers>=4.18.0->-r requirements.txt (line 12)) (6.0.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from transformers>=4.18.0->-r requirements.txt (line 12)) (0.5.3)\n",
      "Requirement already satisfied: psutil in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from accelerate>=0.12.0->-r requirements.txt (line 17)) (7.0.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from datasets>=2.0.0->-r requirements.txt (line 22)) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from datasets>=2.0.0->-r requirements.txt (line 22)) (0.3.8)\n",
      "Requirement already satisfied: xxhash in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from datasets>=2.0.0->-r requirements.txt (line 22)) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from datasets>=2.0.0->-r requirements.txt (line 22)) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.0.0->-r requirements.txt (line 22)) (3.11.18)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from plotly>=5.6.0->-r requirements.txt (line 26)) (1.39.0)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from wandb>=0.12.0->-r requirements.txt (line 29)) (8.2.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from wandb>=0.12.0->-r requirements.txt (line 29)) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from wandb>=0.12.0->-r requirements.txt (line 29)) (3.1.44)\n",
      "Requirement already satisfied: platformdirs in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from wandb>=0.12.0->-r requirements.txt (line 29)) (4.3.8)\n",
      "Requirement already satisfied: pydantic<3 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from wandb>=0.12.0->-r requirements.txt (line 29)) (2.11.4)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from wandb>=0.12.0->-r requirements.txt (line 29)) (2.28.0)\n",
      "Requirement already satisfied: setproctitle in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from wandb>=0.12.0->-r requirements.txt (line 29)) (1.3.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from pydantic<3->wandb>=0.12.0->-r requirements.txt (line 29)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from pydantic<3->wandb>=0.12.0->-r requirements.txt (line 29)) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from pydantic<3->wandb>=0.12.0->-r requirements.txt (line 29)) (0.4.0)\n",
      "Requirement already satisfied: iniconfig in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from pytest>=7.0.0->-r requirements.txt (line 30)) (2.1.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from pytest>=7.0.0->-r requirements.txt (line 30)) (1.5.0)\n",
      "Requirement already satisfied: markdown-it-py>=1.0 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from jupytext>=1.13.8->-r requirements.txt (line 31)) (3.0.0)\n",
      "Requirement already satisfied: mdit-py-plugins in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from jupytext>=1.13.8->-r requirements.txt (line 31)) (0.4.2)\n",
      "Requirement already satisfied: nbformat in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from jupytext>=1.13.8->-r requirements.txt (line 31)) (5.10.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.0.0->-r requirements.txt (line 22)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.0.0->-r requirements.txt (line 22)) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.0.0->-r requirements.txt (line 22)) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.0.0->-r requirements.txt (line 22)) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.0.0->-r requirements.txt (line 22)) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.0.0->-r requirements.txt (line 22)) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.0.0->-r requirements.txt (line 22)) (1.20.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow>=2.8.0->-r requirements.txt (line 10)) (0.45.1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.12.0->-r requirements.txt (line 29)) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.12.0->-r requirements.txt (line 29)) (5.0.2)\n",
      "Requirement already satisfied: rich in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from keras>=3.5.0->tensorflow>=2.8.0->-r requirements.txt (line 10)) (14.0.0)\n",
      "Requirement already satisfied: namex in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from keras>=3.5.0->tensorflow>=2.8.0->-r requirements.txt (line 10)) (0.0.9)\n",
      "Requirement already satisfied: optree in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from keras>=3.5.0->tensorflow>=2.8.0->-r requirements.txt (line 10)) (0.15.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from markdown-it-py>=1.0->jupytext>=1.13.8->-r requirements.txt (line 31)) (0.1.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch>=1.10.0->-r requirements.txt (line 11)) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow>=2.8.0->-r requirements.txt (line 10)) (3.0.2)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from nbformat->jupytext>=1.13.8->-r requirements.txt (line 31)) (2.21.1)\n",
      "Requirement already satisfied: jsonschema>=2.6 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from nbformat->jupytext>=1.13.8->-r requirements.txt (line 31)) (4.23.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from nbformat->jupytext>=1.13.8->-r requirements.txt (line 31)) (5.7.2)\n",
      "Requirement already satisfied: traitlets>=5.1 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from nbformat->jupytext>=1.13.8->-r requirements.txt (line 31)) (5.14.3)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from jsonschema>=2.6->nbformat->jupytext>=1.13.8->-r requirements.txt (line 31)) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from jsonschema>=2.6->nbformat->jupytext>=1.13.8->-r requirements.txt (line 31)) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from jsonschema>=2.6->nbformat->jupytext>=1.13.8->-r requirements.txt (line 31)) (0.24.0)\n",
      "Requirement already satisfied: pywin32>=300 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat->jupytext>=1.13.8->-r requirements.txt (line 31)) (310)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow>=2.8.0->-r requirements.txt (line 10)) (2.19.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (2.7.0)\n",
      "Requirement already satisfied: numpy in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: transformers in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (4.51.3)\n",
      "Requirement already satisfied: datasets in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (3.6.0)\n",
      "Requirement already satisfied: wandb in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (0.19.11)\n",
      "Requirement already satisfied: tqdm in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: filelock in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: setuptools in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from torch) (80.4.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from transformers) (0.31.2)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.18)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from wandb) (8.2.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from wandb) (3.1.44)\n",
      "Requirement already satisfied: platformdirs in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from wandb) (4.3.8)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from wandb) (5.29.4)\n",
      "Requirement already satisfied: psutil>=5.0.0 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from wandb) (7.0.0)\n",
      "Requirement already satisfied: pydantic<3 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from wandb) (2.11.4)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from wandb) (2.28.0)\n",
      "Requirement already satisfied: setproctitle in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from wandb) (1.3.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from pydantic<3->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from pydantic<3->wandb) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from pydantic<3->wandb) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: colorama in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "Requirement already satisfied: six>=1.4.0 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\homework\\ds223\\7-transformers-zhangzwaa\\.venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install -r requirements.txt\n",
    "\n",
    "# Additional packages for nanoGPT\n",
    "%pip install torch numpy transformers datasets wandb tqdm\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('models', exist_ok=True)\n",
    "## 1. Exploring the Synthetic Mention Corpora\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ace664a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Synthetic Mention Corpora from data/synthetic_mentions/SYNTHETIC_MENTIONS.csv\n",
      "        cui                                     matched_output\n",
      "0  C2348412   sig: one (1) tablet po qd. disp:*30 tablet(s)...\n",
      "1  C2348517   4.  pml. 5.   <1CUI> favorable hodgkin lympho...\n",
      "2  C2348639    <1CUI> t waves biphasic </1CUI> .   p waves:...\n",
      "3  C2348499   she was started on hydroxyurea, cyclosporine,...\n",
      "4  C2348501   6.  cytogenic abnormalities: 10/10 metaphases...\n"
     ]
    }
   ],
   "source": [
    "# Let's explore the Synthetic Mention Corpora for Disease Entity Recognition\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "os.makedirs('data/synthetic_mentions', exist_ok=True)\n",
    "\n",
    "# Function to download the dataset\n",
    "def download_synthetic_mentions():\n",
    "    \"\"\"\n",
    "    Download the Synthetic Mention Corpora for Disease Entity Recognition\n",
    "    \n",
    "    Note: You need to manually download this dataset from PhysioNet:\n",
    "    https://physionet.org/content/synthetic-mention-corpora/\n",
    "    \n",
    "    After downloading, place the files in the data/synthetic_mentions directory\n",
    "    \"\"\"\n",
    "    mentions_data_path = 'data/synthetic_mentions/SYNTHETIC_MENTIONS.csv'\n",
    "    \n",
    "    if os.path.exists(mentions_data_path):\n",
    "        print(f\"Loading Synthetic Mention Corpora from {mentions_data_path}\")\n",
    "        data = pd.read_csv(mentions_data_path)\n",
    "        print(data.head())\n",
    "        return data\n",
    "    else:\n",
    "        print(f\"Synthetic Mention Corpora not found at {mentions_data_path}\")\n",
    "        print(\"Please download the dataset from PhysioNet:\")\n",
    "        print(\"https://physionet.org/content/synthetic-mention-corpora/\")\n",
    "        print(\"After downloading, place the files in the data/synthetic_mentions directory\")\n",
    "        return None\n",
    "\n",
    "# Try to load the dataset\n",
    "mentions_data = download_synthetic_mentions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2c13fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 0\n",
      "Total words: 0\n",
      "Total lines: 0\n",
      "\n",
      "First few lines:\n",
      "\n",
      "Warning: The extracted text might be too small for effective training.\n",
      "Consider using more entries from the dataset.\n",
      "Saved combined text to data/processed/mentions_text.txt\n"
     ]
    }
   ],
   "source": [
    "# If the dataset is loaded successfully, convert to text for training\n",
    "if mentions_data is not None:\n",
    "    # Extract mentions and combine into a single text\n",
    "    mentions_text = \"\"\n",
    "    for item in mentions_data[:1000]:  # Start with a subset for exploration\n",
    "        if \"mention\" in item:\n",
    "            mentions_text += item[\"mention\"] + \"\\n\"\n",
    "        if \"context\" in item:\n",
    "            mentions_text += item[\"context\"] + \"\\n\\n\"\n",
    "    \n",
    "    # Print some statistics\n",
    "    print(f\"Total characters: {len(mentions_text)}\")\n",
    "    print(f\"Total words: {len(mentions_text.split())}\")\n",
    "    print(f\"Total lines: {len(mentions_text.splitlines())}\")\n",
    "    \n",
    "    # Print the first few lines\n",
    "    print(\"\\nFirst few lines:\")\n",
    "    for i, line in enumerate(mentions_text.splitlines()[:5]):\n",
    "        print(f\"{i+1}: {line}\")\n",
    "    \n",
    "    # Check if the data is suitable for training\n",
    "    if len(mentions_text) < 100000:  # Less than 100KB\n",
    "        print(\"\\nWarning: The extracted text might be too small for effective training.\")\n",
    "        print(\"Consider using more entries from the dataset.\")\n",
    "    else:\n",
    "        print(\"\\nThe extracted text seems suitable for training.\")\n",
    "    \n",
    "    # Save the combined text for training\n",
    "    with open('data/processed/mentions_text.txt', 'w') as f:\n",
    "        f.write(mentions_text)\n",
    "    print(\"Saved combined text to data/processed/mentions_text.txt\")\n",
    "else:\n",
    "    # Fallback to open_db.txt if the dataset is not available\n",
    "    print(\"Falling back to open_db.txt for training\")\n",
    "    \n",
    "    def read_open_db():\n",
    "        \"\"\"Read the open database text file\"\"\"\n",
    "        with open('open_db.txt', 'r') as f:\n",
    "            text = f.read()\n",
    "        return text\n",
    "    \n",
    "    # Read the open database text\n",
    "    mentions_text = read_open_db()\n",
    "    \n",
    "    # Print some statistics\n",
    "    print(f\"Total characters: {len(mentions_text)}\")\n",
    "    print(f\"Total words: {len(mentions_text.split())}\")\n",
    "    print(f\"Total lines: {len(mentions_text.splitlines())}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703e1190",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Data Preprocessing\n",
    "# Let's preprocess the text data for training\n",
    "\n",
    "class CharacterTokenizer:\n",
    "    \"\"\"Simple character-level tokenizer\"\"\"\n",
    "    \n",
    "    def __init__(self, text):\n",
    "        \"\"\"Initialize the tokenizer with the training text\"\"\"\n",
    "        self.chars = sorted(list(set(text)))\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.stoi = {ch: i for i, ch in enumerate(self.chars)}\n",
    "        self.itos = {i: ch for i, ch in enumerate(self.chars)}\n",
    "        \n",
    "        print(f\"Vocabulary size: {self.vocab_size} characters\")\n",
    "    \n",
    "    def encode(self, text):\n",
    "        \"\"\"Encode text to token IDs\"\"\"\n",
    "        return [self.stoi[ch] for ch in text]\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        \"\"\"Decode token IDs to text\"\"\"\n",
    "        return ''.join([self.itos[id] for id in ids])\n",
    "    \n",
    "    def save(self, path):\n",
    "        \"\"\"Save the tokenizer to a file\"\"\"\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump({\n",
    "                'chars': self.chars,\n",
    "                'vocab_size': self.vocab_size,\n",
    "                'stoi': self.stoi,\n",
    "                'itos': {str(k): v for k, v in self.itos.items()}  # Convert keys to strings for JSON\n",
    "            }, f)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        \"\"\"Load a tokenizer from a file\"\"\"\n",
    "        with open(path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        tokenizer = cls.__new__(cls)\n",
    "        tokenizer.chars = data['chars']\n",
    "        tokenizer.vocab_size = data['vocab_size']\n",
    "        tokenizer.stoi = data['stoi']\n",
    "        tokenizer.itos = {int(k): v for k, v in data['itos'].items()}  # Convert keys back to integers\n",
    "        \n",
    "        return tokenizer\n",
    "\n",
    "# Create a tokenizer from the mentions text\n",
    "tokenizer = CharacterTokenizer(mentions_text)\n",
    "\n",
    "# Encode the entire text\n",
    "encoded_text = tokenizer.encode(mentions_text)\n",
    "print(f\"Encoded text length: {len(encoded_text)} tokens\")\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save('data/processed/char_tokenizer.json')\n",
    "print(\"Tokenizer saved to data/processed/char_tokenizer.json\")\n",
    "\n",
    "# Split the data into train and validation sets (90% train, 10% validation)\n",
    "train_size = int(0.9 * len(encoded_text))\n",
    "train_data = encoded_text[:train_size]\n",
    "val_data = encoded_text[train_size:]\n",
    "\n",
    "print(f\"Train data size: {len(train_data)} tokens\")\n",
    "print(f\"Validation data size: {len(val_data)} tokens\")\n",
    "\n",
    "# Save the processed data\n",
    "np.save('data/processed/train_data.npy', np.array(train_data, dtype=np.int16))\n",
    "np.save('data/processed/val_data.npy', np.array(val_data, dtype=np.int16))\n",
    "print(\"Processed data saved to data/processed/\")\n",
    "\n",
    "# Create a dataset class for training\n",
    "class TextDataset(Dataset):\n",
    "    \"\"\"Dataset for training a language model\"\"\"\n",
    "    \n",
    "    def __init__(self, data, context_length=256):\n",
    "        \"\"\"\n",
    "        Initialize the dataset\n",
    "        \n",
    "        Args:\n",
    "            data: List of token IDs\n",
    "            context_length: Context length for prediction\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.context_length = context_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of possible contexts\"\"\"\n",
    "        return len(self.data) - self.context_length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Get a context and target pair\"\"\"\n",
    "        context = self.data[idx:idx+self.context_length]\n",
    "        target = self.data[idx+1:idx+self.context_length+1]\n",
    "        return torch.tensor(context, dtype=torch.long), torch.tensor(target, dtype=torch.long)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "context_length = 256\n",
    "batch_size = 64\n",
    "\n",
    "train_dataset = TextDataset(train_data, context_length)\n",
    "val_dataset = TextDataset(val_data, context_length)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Number of training batches: {len(train_loader)}\")\n",
    "print(f\"Number of validation batches: {len(val_loader)}\")\n",
    "\n",
    "# Check a sample batch\n",
    "x, y = next(iter(train_loader))\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"Sample input: {tokenizer.decode(x[0].tolist()[:50])}...\")\n",
    "print(f\"Sample target: {tokenizer.decode(y[0].tolist()[:50])}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df37513b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Let's implement a small GPT model (nanoGPT)\n",
    "os.makedirs('results/part_4', exist_ok=True)\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "## 3. Implementing the nanoGPT Model\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-head self-attention module\"\"\"\n",
    "    \n",
    "    def __init__(self, n_embd, n_head, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initialize the multi-head attention module\n",
    "        \n",
    "        Args:\n",
    "            n_embd: Embedding dimension\n",
    "            n_head: Number of attention heads\n",
    "            dropout: Dropout probability\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert n_embd % n_head == 0, \"Embedding dimension must be divisible by number of heads\"\n",
    "        \n",
    "        self.n_embd = n_embd\n",
    "        self.n_head = n_head\n",
    "        self.head_dim = n_embd // n_head\n",
    "        \n",
    "        # Key, query, value projections\n",
    "        self.query = nn.Linear(n_embd, n_embd)\n",
    "        self.key = nn.Linear(n_embd, n_embd)\n",
    "        self.value = nn.Linear(n_embd, n_embd)\n",
    "        \n",
    "        # Output projection\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        \n",
    "        # Regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Causal mask to ensure that attention is only applied to the left\n",
    "        self.register_buffer(\n",
    "            \"mask\", \n",
    "            torch.tril(torch.ones(context_length, context_length))\n",
    "            .view(1, 1, context_length, context_length)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        batch_size, seq_len, n_embd = x.size()\n",
    "        \n",
    "        # Calculate query, key, values\n",
    "        q = self.query(x).view(batch_size, seq_len, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        k = self.key(x).view(batch_size, seq_len, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        v = self.value(x).view(batch_size, seq_len, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        # (batch_size, n_head, seq_len, head_dim) x (batch_size, n_head, head_dim, seq_len)\n",
    "        # -> (batch_size, n_head, seq_len, seq_len)\n",
    "        attn = (q @ k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        \n",
    "        # Apply causal mask\n",
    "        attn = attn.masked_fill(self.mask[:, :, :seq_len, :seq_len] == 0, float('-inf'))\n",
    "        \n",
    "        # Softmax and dropout\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        # (batch_size, n_head, seq_len, seq_len) x (batch_size, n_head, seq_len, head_dim)\n",
    "        # -> (batch_size, n_head, seq_len, head_dim)\n",
    "        out = attn @ v\n",
    "        \n",
    "        # Reshape and project\n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, n_embd)\n",
    "        out = self.proj(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Feed-forward network\"\"\"\n",
    "    \n",
    "    def __init__(self, n_embd, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initialize the feed-forward network\n",
    "        \n",
    "        Args:\n",
    "            n_embd: Embedding dimension\n",
    "            dropout: Dropout probability\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer block\"\"\"\n",
    "    \n",
    "    def __init__(self, n_embd, n_head, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initialize the transformer block\n",
    "        \n",
    "        Args:\n",
    "            n_embd: Embedding dimension\n",
    "            n_head: Number of attention heads\n",
    "            dropout: Dropout probability\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.attn = MultiHeadAttention(n_embd, n_head, dropout)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        self.ff = FeedForward(n_embd, dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class NanoGPT(nn.Module):\n",
    "    \"\"\"Small GPT model\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, n_embd=128, n_head=4, n_layer=4, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initialize the nanoGPT model\n",
    "        \n",
    "        Args:\n",
    "            vocab_size: Size of the vocabulary\n",
    "            n_embd: Embedding dimension\n",
    "            n_head: Number of attention heads\n",
    "            n_layer: Number of transformer blocks\n",
    "            dropout: Dropout probability\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_embd = n_embd\n",
    "        self.n_head = n_head\n",
    "        self.n_layer = n_layer\n",
    "        \n",
    "        # Token embedding\n",
    "        self.token_embedding = nn.Embedding(vocab_size, n_embd)\n",
    "        \n",
    "        # Position embedding\n",
    "        self.position_embedding = nn.Embedding(context_length, n_embd)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(n_embd, n_head, dropout) for _ in range(n_layer)\n",
    "        ])\n",
    "        \n",
    "        # Final layer normalization\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        \n",
    "        # Output head\n",
    "        self.head = nn.Linear(n_embd, vocab_size, bias=False)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "        # Print model size\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        print(f\"NanoGPT model with {n_params/1e6:.2f}M parameters\")\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Initialize weights\"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def forward(self, idx):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        batch_size, seq_len = idx.size()\n",
    "        \n",
    "        # Get token and position embeddings\n",
    "        token_emb = self.token_embedding(idx)\n",
    "        pos = torch.arange(0, seq_len, dtype=torch.long, device=idx.device).unsqueeze(0)\n",
    "        pos_emb = self.position_embedding(pos)\n",
    "        \n",
    "        # Combine embeddings\n",
    "        x = token_emb + pos_emb\n",
    "        \n",
    "        # Apply transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        # Apply final layer norm\n",
    "        x = self.ln_f(x)\n",
    "        \n",
    "        # Apply output head\n",
    "        logits = self.head(x)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0):\n",
    "        \"\"\"\n",
    "        Generate text from the model\n",
    "        \n",
    "        Args:\n",
    "            idx: Starting token IDs (batch_size, seq_len)\n",
    "            max_new_tokens: Maximum number of new tokens to generate\n",
    "            temperature: Temperature for sampling (higher = more random)\n",
    "            \n",
    "        Returns:\n",
    "            Generated token IDs\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_new_tokens):\n",
    "                # Crop context if it's too long\n",
    "                idx_cond = idx if idx.size(1) <= context_length else idx[:, -context_length:]\n",
    "                \n",
    "                # Get predictions\n",
    "                logits = self(idx_cond)\n",
    "                \n",
    "                # Focus on the last token\n",
    "                logits = logits[:, -1, :] / temperature\n",
    "                \n",
    "                # Apply softmax to get probabilities\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                \n",
    "                # Sample from the distribution\n",
    "                idx_next = torch.multinomial(probs, num_samples=1)\n",
    "                \n",
    "                # Append to the sequence\n",
    "                idx = torch.cat((idx, idx_next), dim=1)\n",
    "        \n",
    "        return idx\n",
    "\n",
    "# Create a small nanoGPT model\n",
    "model = NanoGPT(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    n_embd=128,\n",
    "    n_head=4,\n",
    "    n_layer=4,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50818eb4",
   "metadata": {},
   "source": [
    "## 4. Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03be59d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Let's train the nanoGPT model\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs=10, lr=3e-4):\n",
    "    \"\"\"\n",
    "    Train the model\n",
    "    \n",
    "    Args:\n",
    "        model: The model to train\n",
    "        train_loader: Training data loader\n",
    "        val_loader: Validation data loader\n",
    "        epochs: Number of epochs to train for\n",
    "        lr: Learning rate\n",
    "        \n",
    "    Returns:\n",
    "        Training history\n",
    "    \"\"\"\n",
    "    # Initialize optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Initialize learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    \n",
    "    # Initialize loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Initialize training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'train_perplexity': [],\n",
    "        'val_perplexity': []\n",
    "    }\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_batches = 0\n",
    "        \n",
    "        # Progress bar for training\n",
    "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\")\n",
    "        for x, y in train_pbar:\n",
    "            # Move data to device\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(x)\n",
    "            \n",
    "            # Reshape for loss calculation\n",
    "            logits = logits.view(-1, tokenizer.vocab_size)\n",
    "            y = y.view(-1)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(logits, y)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update statistics\n",
    "            train_loss += loss.item()\n",
    "            train_batches += 1\n",
    "            \n",
    "            # Update progress bar\n",
    "            train_pbar.set_postfix({'loss': train_loss / train_batches})\n",
    "        \n",
    "        # Calculate average training loss\n",
    "        avg_train_loss = train_loss / train_batches\n",
    "        train_perplexity = np.exp(avg_train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_batches = 0\n",
    "        \n",
    "        # Progress bar for validation\n",
    "        val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Val]\")\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_pbar:\n",
    "                # Move data to device\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                logits = model(x)\n",
    "                \n",
    "                # Reshape for loss calculation\n",
    "                logits = logits.view(-1, tokenizer.vocab_size)\n",
    "                y = y.view(-1)\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = criterion(logits, y)\n",
    "                \n",
    "                # Update statistics\n",
    "                val_loss += loss.item()\n",
    "                val_batches += 1\n",
    "                \n",
    "                # Update progress bar\n",
    "                val_pbar.set_postfix({'loss': val_loss / val_batches})\n",
    "        \n",
    "        # Calculate average validation loss\n",
    "        avg_val_loss = val_loss / val_batches\n",
    "        val_perplexity = np.exp(avg_val_loss)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "              f\"Train Perplexity: {train_perplexity:.4f}, \"\n",
    "              f\"Val Loss: {avg_val_loss:.4f}, \"\n",
    "              f\"Val Perplexity: {val_perplexity:.4f}\")\n",
    "        \n",
    "        # Update history\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['train_perplexity'].append(train_perplexity)\n",
    "        history['val_perplexity'].append(val_perplexity)\n",
    "        \n",
    "        # Save checkpoint\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'train_loss': avg_train_loss,\n",
    "            'val_loss': avg_val_loss\n",
    "        }, f'models/nanogpt_checkpoint_epoch_{epoch+1}.pt')\n",
    "    \n",
    "    # Save final model\n",
    "    torch.save(model.state_dict(), 'models/nanogpt.pt')\n",
    "    print(\"Model saved to models/nanogpt.pt\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Train the model\n",
    "epochs = 10  # Adjust based on your computational resources\n",
    "history = train_model(model, train_loader, val_loader, epochs=epochs)\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history['train_loss'], label='Train')\n",
    "plt.plot(history['val_loss'], label='Validation')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot perplexity\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history['train_perplexity'], label='Train')\n",
    "plt.plot(history['val_perplexity'], label='Validation')\n",
    "plt.title('Perplexity')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Perplexity')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/part_4/training_history.png')\n",
    "plt.show()\n",
    "\n",
    "# Save training metrics\n",
    "with open('results/part_4/training_metrics.txt', 'w') as f:\n",
    "    f.write(\"# NanoGPT Training Metrics\\n\\n\")\n",
    "    f.write(\"## Model Configuration\\n\")\n",
    "    f.write(f\"Vocabulary Size: {tokenizer.vocab_size}\\n\")\n",
    "    f.write(f\"Embedding Dimension: {model.n_embd}\\n\")\n",
    "## 5. Generating Text and Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aee9689",
   "metadata": {},
   "source": [
    "# Let's generate text from the trained model and evaluate it\n",
    "\n",
    "def generate_text(model, tokenizer, prompt, max_new_tokens=100, temperature=0.8):\n",
    "    \"\"\"\n",
    "    Generate text from the model\n",
    "    \n",
    "    Args:\n",
    "        model: The trained model\n",
    "        tokenizer: The tokenizer\n",
    "        prompt: The prompt text\n",
    "        max_new_tokens: Maximum number of new tokens to generate\n",
    "        temperature: Temperature for sampling (higher = more random)\n",
    "        \n",
    "    Returns:\n",
    "        Generated text\n",
    "    \"\"\"\n",
    "    # Encode the prompt\n",
    "    encoded_prompt = tokenizer.encode(prompt)\n",
    "    \n",
    "    # Convert to tensor and add batch dimension\n",
    "    x = torch.tensor([encoded_prompt], dtype=torch.long).to(device)\n",
    "    \n",
    "    # Generate text\n",
    "    output = model.generate(x, max_new_tokens=max_new_tokens, temperature=temperature)\n",
    "    \n",
    "    # Decode the output\n",
    "    generated_text = tokenizer.decode(output[0].tolist())\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "# Load the trained model\n",
    "model.load_state_dict(torch.load('models/nanogpt.pt'))\n",
    "model.eval()\n",
    "\n",
    "# Generate text with different prompts\n",
    "prompts = [\n",
    "    \"Diabetes is a chronic condition that\",\n",
    "    \"The symptoms of heart disease include\",\n",
    "    \"To prevent respiratory infections,\",\n",
    "    \"The treatment for hypertension typically involves\",\n",
    "    \"Mental health disorders are characterized by\"\n",
    "]\n",
    "\n",
    "# Generate and print text for each prompt\n",
    "print(\"Generated Text Samples:\")\n",
    "for i, prompt in enumerate(prompts):\n",
    "    print(f\"\\nPrompt {i+1}: {prompt}\")\n",
    "    generated_text = generate_text(model, tokenizer, prompt, max_new_tokens=100)\n",
    "    print(f\"Generated: {generated_text}\")\n",
    "\n",
    "# Compare with larger pre-trained models\n",
    "try:\n",
    "    from transformers import pipeline\n",
    "    \n",
    "    # Load a pre-trained model\n",
    "    generator = pipeline('text-generation', model='gpt2')\n",
    "    \n",
    "    print(\"\\n\\nComparison with GPT-2:\")\n",
    "    for i, prompt in enumerate(prompts[:2]):  # Just try a couple of prompts\n",
    "        print(f\"\\nPrompt: {prompt}\")\n",
    "        \n",
    "        # Generate with our nanoGPT\n",
    "        nano_text = generate_text(model, tokenizer, prompt, max_new_tokens=50)\n",
    "        print(f\"NanoGPT: {nano_text}\")\n",
    "        \n",
    "        # Generate with GPT-2\n",
    "        gpt2_text = generator(prompt, max_length=50, num_return_sequences=1)[0]['generated_text']\n",
    "        print(f\"GPT-2: {gpt2_text}\")\n",
    "except:\n",
    "    print(\"\\nSkipping comparison with pre-trained models (requires internet connection)\")\n",
    "\n",
    "# Evaluate the quality of generated text\n",
    "def evaluate_generated_text(generated_samples):\n",
    "    \"\"\"\n",
    "    Evaluate the quality of generated text\n",
    "    \n",
    "    Args:\n",
    "        generated_samples: List of generated text samples\n",
    "        \n",
    "    Returns:\n",
    "        Evaluation metrics\n",
    "    \"\"\"\n",
    "    # Simple metrics for text quality\n",
    "    metrics = {\n",
    "        'avg_length': 0,\n",
    "        'unique_words': 0,\n",
    "        'repetition_rate': 0\n",
    "    }\n",
    "    \n",
    "    total_length = 0\n",
    "    total_unique_words = 0\n",
    "    total_repetition_rate = 0\n",
    "    \n",
    "    for text in generated_samples:\n",
    "        # Calculate length\n",
    "        words = text.split()\n",
    "        length = len(words)\n",
    "        total_length += length\n",
    "        \n",
    "        # Calculate unique words\n",
    "        unique_words = len(set(words))\n",
    "        total_unique_words += unique_words\n",
    "        \n",
    "        # Calculate repetition rate (lower is better)\n",
    "        if length > 0:\n",
    "            repetition_rate = 1 - (unique_words / length)\n",
    "        else:\n",
    "            repetition_rate = 0\n",
    "        total_repetition_rate += repetition_rate\n",
    "    \n",
    "    # Calculate averages\n",
    "    n_samples = len(generated_samples)\n",
    "    if n_samples > 0:\n",
    "        metrics['avg_length'] = total_length / n_samples\n",
    "        metrics['unique_words'] = total_unique_words / n_samples\n",
    "        metrics['repetition_rate'] = total_repetition_rate / n_samples\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Generate a larger set of samples for evaluation\n",
    "evaluation_prompts = [\n",
    "    \"The patient presented with\",\n",
    "    \"Common side effects include\",\n",
    "    \"The diagnosis was confirmed by\",\n",
    "    \"Treatment options for this condition\",\n",
    "    \"The prognosis for patients with\"\n",
    "]\n",
    "\n",
    "generated_samples = []\n",
    "for prompt in evaluation_prompts:\n",
    "    for temp in [0.7, 0.8, 0.9]:  # Try different temperatures\n",
    "        generated_text = generate_text(model, tokenizer, prompt, max_new_tokens=100, temperature=temp)\n",
    "        generated_samples.append(generated_text)\n",
    "\n",
    "# Evaluate the generated samples\n",
    "evaluation_metrics = evaluate_generated_text(generated_samples)\n",
    "print(\"\\nGenerated Text Evaluation:\")\n",
    "for metric, value in evaluation_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Save evaluation results\n",
    "with open('results/part_4/generation_evaluation.txt', 'w') as f:\n",
    "    f.write(\"# NanoGPT Text Generation Evaluation\\n\\n\")\n",
    "    \n",
    "    f.write(\"## Evaluation Metrics\\n\")\n",
    "    for metric, value in evaluation_metrics.items():\n",
    "        f.write(f\"{metric}: {value:.4f}\\n\")\n",
    "    \n",
    "    f.write(\"\\n## Generated Samples\\n\")\n",
    "    for i, (prompt, sample) in enumerate(zip(evaluation_prompts * 3, generated_samples)):\n",
    "        f.write(f\"\\nSample {i+1}:\\n\")\n",
    "        f.write(f\"Prompt: {prompt}\\n\")\n",
    "        f.write(f\"Generated: {sample}\\n\")\n",
    "        f.write(\"-\" * 50 + \"\\n\")\n",
    "\n",
    "print(\"Evaluation results saved to results/part_4/generation_evaluation.txt\")\n",
    "```\n",
    "\n",
    "## Progress Checkpoints\n",
    "\n",
    "1. **Data Exploration**:\n",
    "   - [ ] Download and analyze the Synthetic Mention Corpora\n",
    "   - [ ] Extract disease mentions and contexts\n",
    "   - [ ] Determine if it's suitable for training\n",
    "   - [ ] Prepare the text data for model training\n",
    "\n",
    "2. **Data Preprocessing**:\n",
    "   - [ ] Create a character-level tokenizer\n",
    "   - [ ] Encode the text data\n",
    "   - [ ] Split into train and validation sets\n",
    "\n",
    "3. **Model Implementation**:\n",
    "   - [ ] Implement the nanoGPT architecture\n",
    "   - [ ] Configure model size based on available resources\n",
    "   - [ ] Verify model structure and parameter count\n",
    "\n",
    "4. **Training**:\n",
    "   - [ ] Train the model with appropriate hyperparameters\n",
    "   - [ ] Monitor training progress\n",
    "   - [ ] Save checkpoints and final model\n",
    "\n",
    "5. **Evaluation**:\n",
    "   - [ ] Generate text with different prompts\n",
    "   - [ ] Compare with larger pre-trained models\n",
    "   - [ ] Evaluate text quality metrics\n",
    "   - [ ] Save evaluation results\n",
    "    f.write(f\"Number of Heads: {model.n_head}\\n\")\n",
    "    f.write(f\"Number of Layers: {model.n_layer}\\n\")\n",
    "    f.write(f\"Context Length: {context_length}\\n\")\n",
    "    f.write(f\"Batch Size: {batch_size}\\n\")\n",
    "    f.write(f\"Epochs: {epochs}\\n\\n\")\n",
    "    \n",
    "    f.write(\"## Training Results\\n\")\n",
    "    f.write(f\"Final Train Loss: {history['train_loss'][-1]:.4f}\\n\")\n",
    "    f.write(f\"Final Validation Loss: {history['val_loss'][-1]:.4f}\\n\")\n",
    "    f.write(f\"Final Train Perplexity: {history['train_perplexity'][-1]:.4f}\\n\")\n",
    "    f.write(f\"Final Validation Perplexity: {history['val_perplexity'][-1]:.4f}\\n\\n\")\n",
    "    \n",
    "    f.write(\"## Epoch-by-Epoch Metrics\\n\")\n",
    "    for i in range(epochs):\n",
    "        f.write(f\"Epoch {i+1}:\\n\")\n",
    "        f.write(f\"  Train Loss: {history['train_loss'][i]:.4f}\\n\")\n",
    "        f.write(f\"  Val Loss: {history['val_loss'][i]:.4f}\\n\")\n",
    "        f.write(f\"  Train Perplexity: {history['train_perplexity'][i]:.4f}\\n\")\n",
    "        f.write(f\"  Val Perplexity: {history['val_perplexity'][i]:.4f}\\n\")\n",
    "\n",
    "print(\"Training metrics saved to results/part_4/training_metrics.txt\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
